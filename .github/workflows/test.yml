name: VolGuard Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run full test suite daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Test type to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - stress
          - chaos

env:
  PYTHON_VERSION: '3.10'
  VG_DRY_RUN: 'TRUE'
  VG_ENV: 'TEST'
  UPSTOX_ACCESS_TOKEN: 'test_token_placeholder'
  TELEGRAM_BOT_TOKEN: 'test_bot_token_placeholder'
  TELEGRAM_CHAT_ID: 'test_chat_id_placeholder'

jobs:
  # ========================================
  # LINT & CODE QUALITY
  # ========================================
  lint:
    name: Lint & Code Quality
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy pylint
      
      - name: Run Black (code formatting)
        run: black --check --diff .
        continue-on-error: true
      
      - name: Run isort (import sorting)
        run: isort --check-only --diff .
        continue-on-error: true
      
      - name: Run Flake8 (linting)
        run: |
          flake8 volguard.py --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 volguard.py --count --max-complexity=15 --max-line-length=120 --statistics
        continue-on-error: true
      
      - name: Run Pylint
        run: pylint volguard.py --disable=C,R --errors-only
        continue-on-error: true

  # ========================================
  # UNIT TESTS
  # ========================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'push' || 
      github.event_name == 'pull_request' ||
      github.event.inputs.test_type == 'unit' ||
      github.event.inputs.test_type == 'all'
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist pytest-timeout
          pip install pandas numpy scipy scikit-learn arch
          pip install requests psutil
          pip install upstox-python-sdk
          pip install prometheus-client
      
      - name: Run Unit Tests
        run: |
          pytest test_unit.py \
            -v \
            --cov=volguard \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junit-xml=test-results-unit.xml \
            --tb=short \
            --maxfail=5 \
            -n auto
      
      - name: Upload Coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-unit-${{ matrix.python-version }}
          token: ${{ secrets.CODECOV_TOKEN }}
        continue-on-error: true
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-${{ matrix.python-version }}
          path: |
            test-results-unit.xml
            htmlcov/
      
      - name: Publish Test Report
        uses: mikepenz/action-junit-report@v4
        if: always()
        with:
          report_paths: 'test-results-unit.xml'
          check_name: 'Unit Test Report (Python ${{ matrix.python-version }})'

  # ========================================
  # STRESS TESTS
  # ========================================
  stress-tests:
    name: Stress Tests
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' ||
      github.event.inputs.test_type == 'stress' ||
      github.event.inputs.test_type == 'all'
    
    timeout-minutes: 30
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-timeout pytest-xdist
          pip install pandas numpy scipy scikit-learn arch
          pip install requests psutil
          pip install upstox-python-sdk
          pip install prometheus-client
      
      - name: Run Stress Tests
        run: |
          pytest test_stress.py \
            -v \
            -s \
            -m stress \
            --junit-xml=test-results-stress.xml \
            --tb=short \
            --durations=20 \
            --timeout=300
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: stress-test-results
          path: test-results-stress.xml
      
      - name: Publish Test Report
        uses: mikepenz/action-junit-report@v4
        if: always()
        with:
          report_paths: 'test-results-stress.xml'
          check_name: 'Stress Test Report'

  # ========================================
  # CHAOS TESTS
  # ========================================
  chaos-tests:
    name: Chaos Engineering Tests
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' ||
      github.event.inputs.test_type == 'chaos' ||
      github.event.inputs.test_type == 'all'
    
    timeout-minutes: 20
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-timeout
          pip install pandas numpy scipy scikit-learn arch
          pip install requests psutil
          pip install upstox-python-sdk
          pip install prometheus-client
      
      - name: Run Chaos Tests
        run: |
          pytest test_chaos.py \
            -v \
            -s \
            -m chaos \
            --junit-xml=test-results-chaos.xml \
            --tb=short \
            --timeout=180
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: chaos-test-results
          path: test-results-chaos.xml
      
      - name: Publish Test Report
        uses: mikepenz/action-junit-report@v4
        if: always()
        with:
          report_paths: 'test-results-chaos.xml'
          check_name: 'Chaos Test Report'

  # ========================================
  # SECURITY SCAN
  # ========================================
  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install safety bandit
      
      - name: Run Safety (dependency vulnerabilities)
        run: |
          pip freeze > requirements-freeze.txt
          safety check --file requirements-freeze.txt --output text
        continue-on-error: true
      
      - name: Run Bandit (code security)
        run: |
          bandit -r volguard.py -f json -o bandit-report.json
        continue-on-error: true
      
      - name: Upload Security Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            requirements-freeze.txt

  # ========================================
  # PERFORMANCE BENCHMARKS
  # ========================================
  performance-benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'all'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-benchmark
          pip install pandas numpy scipy scikit-learn arch
          pip install requests psutil
          pip install upstox-python-sdk
          pip install prometheus-client
      
      - name: Run Performance Benchmarks
        run: |
          pytest test_unit.py test_stress.py \
            -v \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-save-data
        continue-on-error: true
      
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: .benchmarks/

  # ========================================
  # TEST SUMMARY
  # ========================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [lint, unit-tests, stress-tests, chaos-tests, security-scan]
    if: always()
    
    steps:
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
      
      - name: Generate Summary
        run: |
          echo "# VolGuard 3.0 Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Lint**: ${{ needs.lint.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Unit Tests**: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Stress Tests**: ${{ needs.stress-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Chaos Tests**: ${{ needs.chaos-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Security Scan**: ${{ needs.security-scan.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "Test reports and coverage data have been uploaded as artifacts." >> $GITHUB_STEP_SUMMARY
      
      - name: Check Overall Status
        run: |
          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
            echo "Unit tests failed!"
            exit 1
          fi

  # ========================================
  # NOTIFY ON FAILURE
  # ========================================
  notify-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [unit-tests, stress-tests, chaos-tests]
    if: failure() && github.event_name == 'schedule'
    
    steps:
      - name: Send Failure Notification
        run: |
          echo "Scheduled test run failed. Check the workflow results."
          # Add your notification logic here (Slack, email, etc.)
